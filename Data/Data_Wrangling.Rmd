


```{r, setup, include=FALSE}
library(tidyverse)
library(robotstxt)
library(rvest)
library(knitr)
library(janitor)
library(rtweet)
library(tidytext)
library(countrycode)
library(rworldmap)
library(viridis)



library(plotly)
library(dplyr)

library(leaflet)
library(gganimate)
library(lubridate)
library(maps)
library(ggthemes)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code

```


#Downloading Data

```{r}

#Downloading data sets
starbucks <- read_csv("starbucks.csv")
city_income <- read_csv("kaggle_income.csv")
city_pop <- read_csv("uscitypopdensity.csv")
county_pop <- read_csv("us_county.csv")


```

#Displaying Starbucks Location:


```{r}

#World Map
worldmap <- getMap(resolution = "coarse")
plot(worldmap, bg = "lightblue", col = "black")
points(starbucks$Longitude, starbucks$Latitude, 
       col = "red", cex = .01)

#US locations; except Hawaii and Alaska
us_stars <- starbucks %>%
  filter(Country=="US") %>%
  rename(State=`State/Province`) %>%
  filter(State!="AK") %>%
  filter(State!="HI")
  
usa <- map_data(map = "state"
                       , region = ".")
usa_states <- usa %>%
  select(region, group) %>%
  group_by(region) %>%
  summarize(
    group=sum(group)/n()
  )

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

us_map_star <- us_stars %>%
  left_join(state_info, by = "State") %>%
  left_join(usa_states, by = c("state_full" = "region"))

us <- map_data(map = "state"
                       , region = ".") 

ggplot() +
  geom_polygon(data=us, aes(x=long, y=lat, group = group),color="blue", fill="black") +
  geom_point(data=us_map_star, aes(x = Longitude, y = Latitude, group=group), color="red") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")

```

#Map of US locations With Magnitude of Starbucks Locations

```{r}

library(tmap)

usa_states <- map_data(map = "state"
                       , region = ".")

city_star <- us_map_star %>%
  group_by(City) %>%
  summarize(
    Count = n()
  )

city_count_star <- us_map_star %>%
  left_join(city_star, by = "City") 


ggplot() +
  geom_polygon(data = usa_states, aes(x=long, y = lat, group = group), color="black", fill="black", alpha=0.3) +
  geom_point(data=city_count_star, aes(x = Longitude, y = Latitude, group=group, size=Count, color=Count)) +
  scale_size_continuous(range=c(1,12)) +
  scale_color_viridis(trans="log") +
  theme_void() 



```


#Collecting and Mapping tweets

```{r}


path_in <- "Users/mattadams/Desktop/Sophomore Year 1st Semester/Data Science/Blog-LetsGetFiscal/Data"

key <- readLines("api_key_twitter.txt")
secret_key <- readLines("api_secret_key.txt")
bearer_token <- readLines("api_bearer_token.txt")
access_token <- readLines("access_token.txt")
secret_access_token <- readLines("secret_access_token.txt")

#setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)


data(stop_words)
https <- rbind(stop_words, c("https", "SMART"))
t.co <- rbind(https, c("t.co", "SMART"))
stop_words_2 <- t.co

nrc_lexicon <- get_sentiments("nrc")

tweets <- search_tweets("starbucks", n = 1000, lang = "en") %>%
  select(status_id, text, location) %>%
  filter(!is.na(location))

tweet_locations <- tweets %>%
  separate(location, into = c("City", "State")
           , sep = ","
           , remove = FALSE) %>%
  select(status_id, text, State) %>%
  na.omit
  
#Before I create the one word per row dataset, group_by location and id, then it will keep tweet ID variable in

tweet_words0 <- tweet_locations %>%
  group_by(status_id, State) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words_2, by="word")
  
tweet_words <- tweet_words0 %>%
  group_by(State) %>%
  summarize(
    Total_Words = n(), word=word, State=State, status_id = status_id
  )

positive <- nrc_lexicon %>%
  filter(sentiment=="positive")

negative <- nrc_lexicon %>%
  filter(sentiment=="negative")

tweet_positive <- tweet_words %>%
  inner_join(positive, by="word") 

tweet_negative <- tweet_words %>%
  inner_join(negative, by="word")

state_info <- data.frame(state_full = tolower(state.name)
                         , State.abb = state.abb
                         , State.name = state.name
                         , Region = state.region)

word_count <- tweet_words %>%
  select(State, Total_Words) %>%
  group_by(State) %>%
  summarize(
    N = n()
  )

starbucks_positive <- tweet_positive %>%
  mutate(State=str_trim(State)) %>%
  group_by(State) %>%
  summarize (
    positive.proportion = n()/(sum(Total_Words)/n())
  ) %>%
  inner_join(state_info, by= c("State"="State.abb")) %>%
  right_join(usa_states, by = c("state_full" = "region"))

#Plotting positive across US
ggplot(starbucks_positive, aes(x = long, y = lat, group = group
                      , fill = positive.proportion)) +
  scale_fill_viridis() +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of Positive Words in Tweets Per State",
       caption = "*Grey states have no tweet information") +
  theme(legend.position="bottom") 
  
#Plotting negative across US
starbucks_negative <- tweet_negative %>%
  mutate(State=str_trim(State)) %>%
  group_by(State) %>%
  summarize (
    negative.proportion = n()/(sum(Total_Words)/n())
  ) %>%
  inner_join(state_info, by= c("State"="State.abb")) %>%
  right_join(usa_states, by = c("state_full" = "region"))

ggplot(starbucks_negative, aes(x = long, y = lat, group = group
                      , fill = negative.proportion)) +
  scale_fill_viridis() +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of Negative Words in Tweets Per State",
       caption = "*Grey states have no tweet information") +
  theme(legend.position="bottom") 
  




```


#City Population Density Map


```{r}

#City Population
city_pop_1 <- city_pop %>%
  rename(pop_dens = `Population Density (Persons/Square Mile)`)

usa_states <- map_data(map = "state"
                       , region = ".")

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.name
                         , Region = state.region)

city_pop_map <- city_pop_1 %>%
  inner_join(state_info, by = "State") %>%
  inner_join(usa_states, by = c("state_full" = "region"))

ggplot(city_pop_map, aes(x = long, y = lat, group = group
                      , fill = pop_dens)) +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")

#Change colors

```


#State-wide population 

```{r}

clean_county <- county_pop %>%
  separate(county, into = c("County", "Blank")
           , sep = "County"
           , remove = FALSE) %>%
  separate(County, into = c("County", "Blank")
           , sep = "Census Area"
           , remove = FALSE) %>%
  select(County, state_code, population, long, lat) %>%
  mutate(County=tolower(County)) %>%
  mutate(County=str_trim(County))

state_pop <- clean_county %>%
  group_by(state_code) %>%
  summarize(
    pop = sum(population)
  )
  
  
usa_states <- map_data(map = "state"
                       , region = ".")

#usa_states <- usa_states0 %>%
 # group_by(subregion) %>%
 # summarize(
  #  group = mean(group)
  #) %>%
  #mutate(subregion=str_trim(subregion))


state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

#WHY DOES IT ALWAYS INCREASE ROWS
#HOW CAN I FIX VISUALIZATION
county_population <- state_pop %>%
  left_join(state_info, by= c("state_code"="State")) %>%
  left_join(usa_states, by = c("state_full" = "region"))

ggplot() +
  geom_polygon(data=county_population, aes(x = long, y = lat, group=group, fill=population)) +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")


```


#Fast Food Chains Per Population (Creating a Fast Food Chain Density Metric)

```{r}

restaurants_us <- restaurants %>%
  filter(country == "US")

#Adding states to restaurants locations
city <- data.frame(city = us.cities) 
  
us_cities <- city %>%
  separate(city.name, into = c("city", "State")
           , sep = " "
           , remove = FALSE) 

rest_city <- restaurants_us %>%
  left_join(us_cities, by = "city") %>%
  rename(lat=latitude, long=longitude)

#Sum of Amount of Fast Food Chains in a city
fast_food_density <- rest_city %>%
  mutate(`Amount of Fast Food`=1) %>%
  group_by(State) %>%
  summarize(
    fast_density = sum(`Amount of Fast Food`)
  ) %>%
  na.omit

usa_states <- map_data(map = "state"
                       , region = ".")

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

restaurant_density_map <- fast_food_density %>%
  left_join(state_info, by = "State") %>%
  left_join(usa_states, by = c("state_full" = "region"))



ggplot(restaurant_density_map, aes(x = long, y = lat, group = group
                      , fill = fast_density)) +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") 
  


```



