


```{r, setup, include=FALSE}
library(tidyverse)
library(robotstxt)
library(rvest)
library(knitr)
library(janitor)
library(rtweet)
library(tidytext)
library(countrycode)
library(rworldmap)


library(plotly)
library(dplyr)

library(leaflet)
library(gganimate)
library(lubridate)
library(maps)
library(ggthemes)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code

```


#Downloading Data

```{r}

#Downloading data sets
starbucks <- read_csv("starbucks.csv")
city_income <- read_csv("kaggle_income.csv")
city_pop <- read_csv("uscitypopdensity.csv")

worldmap <- getMap(resolution = "coarse")

plot(worldmap, bg = "lightblue", col = "black")
points(starbucks$Longitude, starbucks$Latitude, 
       col = "red", cex = .01)


us_stars <- starbucks %>%
  filter(Country=="US") %>%
  rename(State=`State/Province`) %>%
  filter(State!="AK") %>%
  filter(State!="HI")
  
usa <- map_data(map = "state"
                       , region = ".")
usa_states <- usa %>%
  select(region, group) %>%
  group_by(region) %>%
  summarize(
    group=sum(group)/n()
  )

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

us_map_star <- us_stars %>%
  left_join(state_info, by = "State") %>%
  left_join(usa_states, by = c("state_full" = "region"))

us <- map_data(map = "state"
                       , region = ".") 

ggplot() +
  geom_polygon(data=us, aes(x=long, y=lat, group = group),color="blue", fill="black" ) +
  geom_point(data=us_map_star, aes(x = Longitude, y = Latitude, group=group), color="red") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")

```

#Displaying Starbucks Location:


```{r}

#World Map
worldmap <- getMap(resolution = "coarse")
plot(worldmap, bg = "lightblue", col = "black")
points(starbucks$Longitude, starbucks$Latitude, 
       col = "red", cex = .01)

#US locations; except Hawaii and Alaska
us_stars <- starbucks %>%
  filter(Country=="US") %>%
  rename(State=`State/Province`) %>%
  filter(State!="AK") %>%
  filter(State!="HI")
  
usa <- map_data(map = "state"
                       , region = ".")
usa_states <- usa %>%
  select(region, group) %>%
  group_by(region) %>%
  summarize(
    group=sum(group)/n()
  )

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

us_map_star <- us_stars %>%
  left_join(state_info, by = "State") %>%
  left_join(usa_states, by = c("state_full" = "region"))

us <- map_data(map = "state"
                       , region = ".") 

ggplot() +
  geom_polygon(data=us, aes(x=long, y=lat, group = group),color="blue", fill="black" ) +
  geom_point(data=us_map_star, aes(x = Longitude, y = Latitude, group=group), color="red") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")

```


#Collecting tweets

```{r}


path_in <- "Users/mattadams/Desktop/Sophomore Year 1st Semester/Data Science/Blog-LetsGetFiscal/Data"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)


key <- readLines("api_key_twitter.txt")
secret_key <- readLines("api_secret_key.txt")
bearer_token <- readLines("api_bearer_token.txt")
access_token <- readLines("access_token.txt")
secret_access_token <- readLines("secret_access_token.txt")


data(stop_words)

nrc_lexicon <- get_sentiments("nrc")

mcds <- search_users(q="#mcdonald",
                      n = 50) %>%
  filter(lang=="en")

wendys <- search_users(q="#wendys",
                      n = 50) %>%
  filter(lang=="en")

burgerking <- search_users(q="#burgerking",
                      n = 50) %>%
  filter(lang=="en")

tweets <- search_tweets("mcdonalds", n = 100, lang = "en") %>%
  select(status_id, text, location) %>%
  filter(!is.na(location))

tweet_locations <- tweets %>%
  separate(location, into = c("City", "State")
           , sep = ","
           , remove = FALSE) %>%
  select(status_id, text, State) %>%
  na.omit
  

#NEED:
  #Can we shade in densities by state with a general lat and long?
    #Sem-join 
  #Can we get a sentiment analysis of a group of text that is still attached to its location?

https <- rbind(stop_words, c("https", "SMART"))
t.co <- rbind(https, c("t.co", "SMART"))
stop_words_2 <- t.co

#Before I create the one word per row dataset, group_by location and id, then it will keep tweet ID variable in
tweet_words <- tweet_locations %>%
  group_by(status_id, State) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words_2, by="word")

positive <- nrc_lexicon %>%
  filter(sentiment=="positive")

negative <- nrc_lexicon %>%
  filter(sentiment=="negative")

tweet_positive <- tweet_words %>%
  semi_join(positive, by="word")


```


#City Population Density Map


```{r}

#City Population
city_pop_1 <- city_pop %>%
  rename(pop_dens = `Population Density (Persons/Square Mile)`)

usa_states <- map_data(map = "state"
                       , region = ".")

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.name
                         , Region = state.region)

city_pop_map <- city_pop_1 %>%
  left_join(state_info, by = "State") %>%
  right_join(usa_states, by = c("state_full" = "region"))

ggplot(city_pop_map, aes(x = long, y = lat, group = group
                      , fill = pop_dens)) +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") +
  scale_fill_distiller(palette = "BuPu", direction = "horizantle")

#Change colors

```


#City Income 

```{r}

city_income_1 <- city_income %>%
  rename(State=State_Name) %>%
  select(State, Median)

usa_states <- map_data(map = "state"
                       , region = ".")

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.name
                         , Region = state.region)

city_income_1 <- city_income_1 %>%
  left_join(state_info, by = "State") %>%
  right_join(usa_states, by = c("state_full" = "region"))

ggplot(city_income_1, aes(x = long, y = lat, group = group
                      , fill = Median)) +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom")


```


#Fast Food Chains Per Population (Creating a Fast Food Chain Density Metric)

```{r}

restaurants_us <- restaurants %>%
  filter(country == "US")

#Adding states to restaurants locations
city <- data.frame(city = us.cities) 
  
us_cities <- city %>%
  separate(city.name, into = c("city", "State")
           , sep = " "
           , remove = FALSE) 

rest_city <- restaurants_us %>%
  left_join(us_cities, by = "city") %>%
  rename(lat=latitude, long=longitude)

#Sum of Amount of Fast Food Chains in a city
fast_food_density <- rest_city %>%
  mutate(`Amount of Fast Food`=1) %>%
  group_by(State) %>%
  summarize(
    fast_density = sum(`Amount of Fast Food`)
  ) %>%
  na.omit

usa_states <- map_data(map = "state"
                       , region = ".")

state_info <- data.frame(state_full = tolower(state.name)
                         , State = state.abb
                         , Region = state.region)

restaurant_density_map <- fast_food_density %>%
  left_join(state_info, by = "State") %>%
  right_join(usa_states, by = c("state_full" = "region"))



ggplot(restaurant_density_map, aes(x = long, y = lat, group = group
                      , fill = fast_density)) +
  geom_polygon(color = "white") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  labs(fill = "Proportion of colleges planning for in-person") +
  theme(legend.position="bottom") 
  


```



